{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce5e63d4-b7d4-4221-825a-d81ceac16769",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-07T14:09:37.833053Z",
     "iopub.status.busy": "2023-05-07T14:09:37.832883Z",
     "iopub.status.idle": "2023-05-07T14:09:37.836195Z",
     "shell.execute_reply": "2023-05-07T14:09:37.835687Z",
     "shell.execute_reply.started": "2023-05-07T14:09:37.833039Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b25165dc-bffb-42b4-9895-74b66db1f135",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-07T13:59:12.226039Z",
     "iopub.status.busy": "2023-05-07T13:59:12.225906Z",
     "iopub.status.idle": "2023-05-07T13:59:12.229526Z",
     "shell.execute_reply": "2023-05-07T13:59:12.229020Z",
     "shell.execute_reply.started": "2023-05-07T13:59:12.226026Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from train import train_epoch, eval_epoch\n",
    "from model import MegaSiameseModel, MegaSiameseModelv2\n",
    "from dataset import TopG_Dataset, topg_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c68175a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 777\n",
    "\n",
    "\n",
    "def seed_everything(seed: int,\n",
    "                    use_deterministic_algos: bool = False) -> None:\n",
    "    \n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.use_deterministic_algorithms(use_deterministic_algos)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    \n",
    "seed_everything(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2d826da-a1f1-4b4f-97fa-4b6dbd3070f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-07T13:59:12.230274Z",
     "iopub.status.busy": "2023-05-07T13:59:12.230018Z",
     "iopub.status.idle": "2023-05-07T13:59:12.305068Z",
     "shell.execute_reply": "2023-05-07T13:59:12.304567Z",
     "shell.execute_reply.started": "2023-05-07T13:59:12.230260Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def single_model(model, \n",
    "                     dataset, \n",
    "                     loss_function,\n",
    "                     collate_fn,\n",
    "                     device=torch.device(\"cuda\"),\n",
    "                     random_state: int=69, \n",
    "                     shuffle=True,\n",
    "                     epochs: int=15, \n",
    "                     lr: float=1e-6,\n",
    "                     batch_size: int=32,\n",
    "                     start_epoch=0,\n",
    "                     ):\n",
    "    random.seed(random_state),\n",
    "    np.random.seed(random_state)\n",
    "    torch.manual_seed(random_state)\n",
    "    torch.cuda.manual_seed_all(random_state)\n",
    "    \n",
    "    loss_function.to(device)\n",
    "    model.to(device)\n",
    "    optimizer = AdamW(\n",
    "        [\n",
    "            {\"params\": model.out.parameters(), \"lr\": 1e-4},\n",
    "            {\"params\": model.bert1.parameters(), \"lr\": 1e-5},\n",
    "            {\"params\": model.bert2.parameters(), \"lr\": 1e-5}\n",
    "        ]            \n",
    "    )\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "                    dataset,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=shuffle,\n",
    "                     collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    total_steps = len(data_loader) * epochs \n",
    "\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "    for epoch_i in range(0, epochs):\n",
    "        if epoch_i >= start_epoch:\n",
    "            train_metrics = train_epoch(model, data_loader, loss_function, optimizer, scheduler, device)\n",
    "            print(\"EPOCH\", epoch_i)\n",
    "            print(train_metrics)\n",
    "            #eval_metrics = eval_epoch(model, eval_loader, loss_function, device)\n",
    "            #print(eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6ad9845-a447-4fda-b99d-75c54ef516de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-07T13:59:12.306019Z",
     "iopub.status.busy": "2023-05-07T13:59:12.305576Z",
     "iopub.status.idle": "2023-05-07T13:59:12.400775Z",
     "shell.execute_reply": "2023-05-07T13:59:12.400319Z",
     "shell.execute_reply.started": "2023-05-07T13:59:12.305998Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA = \"data/\"\n",
    "train = pd.read_csv(f\"{DATA}train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "600ae180-7c32-4dc6-ac59-76e44a4a22f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-07T13:59:12.401728Z",
     "iopub.status.busy": "2023-05-07T13:59:12.401307Z",
     "iopub.status.idle": "2023-05-07T13:59:14.686339Z",
     "shell.execute_reply": "2023-05-07T13:59:14.685941Z",
     "shell.execute_reply.started": "2023-05-07T13:59:12.401712Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ai-forever/ruRoberta-large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at ai-forever/ruRoberta-large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\"ai-forever/ruRoberta-large\"\n",
    "\"RussianNLP/ruRoBERTa-large-rucola\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/ruRoberta-large\")\n",
    "\n",
    "#model = AutoModelForPreTraining.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "bert1 = AutoModel.from_pretrained(\"ai-forever/ruRoberta-large\")\n",
    "bert2 = AutoModel.from_pretrained(\"ai-forever/ruRoberta-large\")\n",
    "model = MegaSiameseModel(bert1, bert2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2bda583-67a3-4651-8b0e-f9e41048793d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-07T13:59:14.687040Z",
     "iopub.status.busy": "2023-05-07T13:59:14.686841Z",
     "iopub.status.idle": "2023-05-07T13:59:16.664858Z",
     "shell.execute_reply": "2023-05-07T13:59:16.664362Z",
     "shell.execute_reply.started": "2023-05-07T13:59:14.687023Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = TopG_Dataset(train, tokenizer, 200, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe40f0d7-1f64-447a-91a7-1ec5b6d2dbc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-07T14:17:28.312694Z",
     "iopub.status.busy": "2023-05-07T14:17:28.312449Z",
     "iopub.status.idle": "2023-05-07T14:17:28.318314Z",
     "shell.execute_reply": "2023-05-07T14:17:28.317927Z",
     "shell.execute_reply.started": "2023-05-07T14:17:28.312669Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cross_validation(project_name,\n",
    "                     model, \n",
    "                     dataset, \n",
    "                     loss_function,\n",
    "                     collate_fn,\n",
    "                     strat_array=None,\n",
    "                     device=torch.device(\"cuda\"),\n",
    "                     random_state: int=69, \n",
    "                     shuffle: bool=True, \n",
    "                     n_folds: int=4, \n",
    "                     epochs: int=5, \n",
    "#                     lr: float=1e-6,\n",
    "                     start_fold: int=0, \n",
    "                     batch_size: int=32,\n",
    "                     iters_to_accumulate=None,\n",
    "                     n_accumulated_grads: int = 0):\n",
    "    random.seed(random_state),\n",
    "    np.random.seed(random_state)\n",
    "    torch.manual_seed(random_state)\n",
    "    torch.cuda.manual_seed_all(random_state)\n",
    "    \n",
    "    loss_function.to(device)\n",
    "    if strat_array:\n",
    "        kfold = StratifiedKFold(n_folds, shuffle=shuffle, random_state=random_state)\n",
    "        split = kfold.split(dataset, strat_array)\n",
    "    else: \n",
    "        kfold = KFold(n_folds, shuffle=shuffle, random_state=random_state)\n",
    "        split = kfold.split(dataset)\n",
    "    os.mkdir(f\"models/{project_name}/\")\n",
    "    for fold, (train_ids, eval_ids) in enumerate(split):\n",
    "        if fold >= start_fold:\n",
    "            print(f'FOLD {fold}')\n",
    "            print('--------------------------------')\n",
    "            \n",
    "            fold_model = deepcopy(model)\n",
    "            fold_optimizer = AdamW(\n",
    "                [\n",
    "                    {\"params\": fold_model.out.parameters(), \"lr\": 3e-4},\n",
    "                    {\"params\": fold_model.bert1.parameters(), \"lr\": 1e-5},\n",
    "                    {\"params\": fold_model.bert2.parameters(), \"lr\": 1e-5}\n",
    "                ]            \n",
    "            )\n",
    "\n",
    "            train_subsampler = torch.utils.data.Subset(dataset,  train_ids)\n",
    "            train_loader = torch.utils.data.DataLoader(\n",
    "                          train_subsampler, \n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=shuffle,collate_fn=collate_fn,drop_last=True)\n",
    "\n",
    "            eval_subsampler = torch.utils.data.Subset(dataset,  eval_ids)\n",
    "            eval_loader = torch.utils.data.DataLoader(\n",
    "                          eval_subsampler,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=shuffle,collate_fn=collate_fn,drop_last=True)\n",
    "            \n",
    "            total_steps = len(train_loader) * epochs \n",
    "\n",
    "            fold_scheduler = get_cosine_schedule_with_warmup(fold_optimizer, \n",
    "                                                    num_warmup_steps = 10, # Default value in run_glue.py\n",
    "                                                    num_training_steps = total_steps)\n",
    "\n",
    "            mrrs = []\n",
    "\n",
    "            for epoch_i in range(0, epochs):\n",
    "                train_metrics = train_epoch(fold_model, train_loader, loss_function, fold_optimizer, fold_scheduler, device)\n",
    "                eval_metrics = eval_epoch(fold_model, eval_loader, loss_function, device)\n",
    "                \n",
    "                print(f\"EPOCH: {epoch_i}\")\n",
    "                print(train_metrics)\n",
    "                print(eval_metrics)\n",
    "                \n",
    "                #run.log(train_metrics)\n",
    "                #run.log(eval_metrics)\n",
    "            torch.save(fold_model, f\"models/{project_name}/fold_{fold}.pt\")\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65da88dc-4f07-4e6a-b765-ae72996a2adb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-07T14:17:29.269953Z",
     "iopub.status.busy": "2023-05-07T14:17:29.269787Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cross_validation(\n",
    "#     \"A\", \n",
    "#     model = model,\n",
    "#     dataset=dataset, \n",
    "#     loss_function=nn.CrossEntropyLoss(), \n",
    "#     collate_fn=topg_collate,\n",
    "#     batch_size=16,\n",
    "\n",
    "# #             lr=3e-4,\n",
    "#     epochs=5,\n",
    "#     device=torch.device(\"cuda\"),\n",
    "#     random_state=69,\n",
    "#     shuffle=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72b1416c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c165084ce13541a092f4f3e560bec68c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/385 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leffff/PycharmProjects/rucode_7/dataset.py:58: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  context = torch.tensor(context).squeeze()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0\n",
      "{'Train Loss': 0.3852034061373054, 'Train Accuracy': 0.8497804999351501, 'Train F1*100': 84.44444444444444}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c03347baa404a6d9967e968bc19ac57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/385 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1\n",
      "{'Train Loss': 0.14922626252685275, 'Train Accuracy': 0.9681352376937866, 'Train F1*100': 96.74418604651163}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92cd8f5453364de18735ac555e481eec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/385 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 2\n",
      "{'Train Loss': 0.09036046719638172, 'Train Accuracy': 0.9842302203178406, 'Train F1*100': 98.39695918030078}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f6e91bc2aca41999d5150c9656b5c1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/385 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 3\n",
      "{'Train Loss': 0.05897172284252071, 'Train Accuracy': 0.9904080629348755, 'Train F1*100': 99.02463217060672}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "181d233d1ee44c1e9e6f0a73f09d3ec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/385 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msingle_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtopg_collate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;43;03m#             lr=3e-4,\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 43\u001b[0m, in \u001b[0;36msingle_model\u001b[0;34m(model, dataset, loss_function, collate_fn, device, random_state, shuffle, epochs, lr, batch_size, start_epoch)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, epochs):\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch_i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m start_epoch:\n\u001b[0;32m---> 43\u001b[0m         train_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPOCH\u001b[39m\u001b[38;5;124m\"\u001b[39m, epoch_i)\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;28mprint\u001b[39m(train_metrics)\n",
      "File \u001b[0;32m~/PycharmProjects/rucode_7/train.py:29\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, data_loader, loss_function, optimizer, scheduler, device)\u001b[0m\n\u001b[1;32m     26\u001b[0m targets\u001b[38;5;241m.\u001b[39mappend(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     28\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(logits, batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 29\u001b[0m total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     32\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "single_model(\n",
    "    model = model,\n",
    "    dataset=dataset, \n",
    "    loss_function=nn.CrossEntropyLoss(), \n",
    "    collate_fn=topg_collate,\n",
    "    batch_size=16,\n",
    "\n",
    "#             lr=3e-4,\n",
    "    epochs=7,\n",
    "    device=torch.device(\"cuda\"),\n",
    "    random_state=random_state,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e0697b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, f\"models/rrl_{random_state}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49ea7d94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1631</th>\n",
       "      <td>people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1632</th>\n",
       "      <td>people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1633</th>\n",
       "      <td>people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1634</th>\n",
       "      <td>people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1635</th>\n",
       "      <td>people</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1636 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label\n",
       "0     people\n",
       "1     people\n",
       "2     people\n",
       "3     people\n",
       "4     people\n",
       "...      ...\n",
       "1631  people\n",
       "1632  people\n",
       "1633  people\n",
       "1634  people\n",
       "1635  people\n",
       "\n",
       "[1636 rows x 1 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(f\"{DATA}public_test.csv\")\n",
    "sub_df = pd.read_csv(f\"{DATA}sample_submission.csv\")\n",
    "sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69422653",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[MegaSiameseModel(\n",
       "   (bert1): RobertaModel(\n",
       "     (embeddings): RobertaEmbeddings(\n",
       "       (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "       (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "       (token_type_embeddings): Embedding(1, 1024)\n",
       "       (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (encoder): RobertaEncoder(\n",
       "       (layer): ModuleList(\n",
       "         (0-23): 24 x RobertaLayer(\n",
       "           (attention): RobertaAttention(\n",
       "             (self): RobertaSelfAttention(\n",
       "               (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "               (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "               (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "             (output): RobertaSelfOutput(\n",
       "               (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "               (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): RobertaIntermediate(\n",
       "             (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "             (intermediate_act_fn): GELUActivation()\n",
       "           )\n",
       "           (output): RobertaOutput(\n",
       "             (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "             (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (pooler): RobertaPooler(\n",
       "       (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "       (activation): Tanh()\n",
       "     )\n",
       "   )\n",
       "   (bert2): RobertaModel(\n",
       "     (embeddings): RobertaEmbeddings(\n",
       "       (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "       (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "       (token_type_embeddings): Embedding(1, 1024)\n",
       "       (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (encoder): RobertaEncoder(\n",
       "       (layer): ModuleList(\n",
       "         (0-23): 24 x RobertaLayer(\n",
       "           (attention): RobertaAttention(\n",
       "             (self): RobertaSelfAttention(\n",
       "               (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "               (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "               (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "             (output): RobertaSelfOutput(\n",
       "               (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "               (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): RobertaIntermediate(\n",
       "             (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "             (intermediate_act_fn): GELUActivation()\n",
       "           )\n",
       "           (output): RobertaOutput(\n",
       "             (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "             (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (pooler): RobertaPooler(\n",
       "       (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "       (activation): Tanh()\n",
       "     )\n",
       "   )\n",
       "   (out): Linear(in_features=2048, out_features=2, bias=True)\n",
       " )]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = []\n",
    "random_states = [777]\n",
    "for rs in random_states:\n",
    "    models.append(torch.load(f\"models/rrl_{rs}.pt\"))\n",
    "\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f0a1fe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2222906b531c4b0e90c2d411a8309d00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import predict, predict_ensemble\n",
    "\n",
    "predict_ensemble(\n",
    "    models, \n",
    "    tokenizer, \n",
    "    test_df, \n",
    "    sub_df, \n",
    "    path_to_save=\"submission_rrl_777_check.csv\",\n",
    "    context_len=200,\n",
    "    answer_len=60,\n",
    "    batch_size=64,\n",
    "    device=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3a6bb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
